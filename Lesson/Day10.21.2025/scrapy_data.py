import scrapy
import csv
import json
from datetime import datetime
from scrapy.crawler import CrawlerProcess
import os
print("Directory:", os.getcwd())
os.chdir(r"C:\Users\Admin\Desktop\PDS301m_HuongNTC2\PDS301m_HuongNTC2-\Lesson\Day10.21.2025")
# ----- C√ÅCH 1: SPIDER N√ÇNG CAO T·ª∞ L∆ØU FILE -----
class AdvancedQuotesSpider(scrapy.Spider):
    name = "advanced_quotes"
    start_urls = ['http://quotes.toscrape.com/tag/humor/']

    def __init__(self):
        """Kh·ªüi t·∫°o spider, t·∫°o file v√† ghi header"""
        super().__init__()
        self.csv_file = 'quotes_data.csv'
        self.json_file = 'quotes_data.json'
        self.data_count = 0

        # Kh·ªüi t·∫°o CSV v·ªõi header
        with open(self.csv_file, 'w', newline='', encoding='utf-8') as file:
            writer = csv.writer(file)
            writer.writerow(['ID', 'Quote', 'Author', 'Tags', 'URL', 'Timestamp'])

        # Kh·ªüi t·∫°o JSON file
        with open(self.json_file, 'w', encoding='utf-8') as file:
            json.dump([], file, ensure_ascii=False, indent=4)

    def save_to_csv(self, data):
        """L∆∞u d·ªØ li·ªáu v√†o CSV"""
        with open(self.csv_file, 'a', newline='', encoding='utf-8') as file:
            writer = csv.writer(file)
            writer.writerow([
                data['id'],
                data['quote'],
                data['author'],
                ', '.join(data['tags']), # Join list tags th√†nh string
                data['url'],
                data['timestamp']
            ])

    def save_to_json(self, data):
        """L∆∞u d·ªØ li·ªáu v√†o JSON"""
        with open(self.json_file, 'r+', encoding='utf-8') as file:
            existing_data = json.load(file)
            existing_data.append(data)
            file.seek(0) # Quay v·ªÅ ƒë·∫ßu file
            json.dump(existing_data, file, ensure_ascii=False, indent=4)

    def closed(self, reason):
        """ƒê∆∞·ª£c g·ªçi khi spider k·∫øt th√∫c"""
        print(f"üï∑Ô∏è Spider ƒë√£ k·∫øt th√∫c: {reason}")
        print(f"üìä T·ªïng s·ªë quotes ƒë√£ crawl: {self.data_count}")

    def parse(self, response):
        """H√†m parse ch√≠nh ƒë·ªÉ tr√≠ch xu·∫•t d·ªØ li·ªáu"""
        for quote in response.css('div.quote'):
            self.data_count += 1
            quote_data = {
                'id': self.data_count,
                'quote': quote.css('span.text::text').get(),
                'author': quote.css('small.author::text').get(),
                'tags': quote.css('div.tags a.tag::text').getall(),
                'url': response.url,
                'timestamp': datetime.now().isoformat()
            }
            
            # L∆∞u v√†o c·∫£ CSV v√† JSON
            self.save_to_csv(quote_data)
            self.save_to_json(quote_data)
            
            yield quote_data

        # Pagination (Ph√¢n trang)
        next_page = response.css('li.next a::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)


# ----- C√ÅCH 2: S·ª¨ D·ª§NG BUILT-IN FEEDS C·ª¶A SCRAPY -----
class QuotesSpiderWithFeeds(scrapy.Spider):
    name = "quotes_feeds"
    start_urls = ['http://quotes.toscrape.com/tag/humor/']

    # C·∫•u h√¨nh ƒë·ªÉ Scrapy t·ª± ƒë·ªông xu·∫•t file
    custom_settings = {
        'FEEDS': {
            'quotes.csv': {
                'format': 'csv',
                'fields': ['quote', 'author', 'tags', 'url'], # Ch·ªçn c√°c tr∆∞·ªùng
                'encoding': 'utf8',
            },
            'quotes.json': {
                'format': 'json',
                'encoding': 'utf8',
                'indent': 4,
            },
        },
        'FEED_EXPORT_ENCODING': 'utf-8',
    }

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'quote': quote.css('span.text::text').get(),
                'author': quote.css('small.author::text').get(),
                'tags': ', '.join(quote.css('div.tags a.tag::text').getall()),
                'url': response.url,
            }
        
        # Pagination (Ph√¢n trang)
        next_page = response.css('li.next a::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)


# ----- KH·ªêI L·ªÜNH ƒê·ªÇ CH·∫†Y SPIDER -----
if __name__ == "__main__":
    print("üöÄ B·∫Øt ƒë·∫ßu crawling quotes...")
    
    # C·∫•u h√¨nh process
    process = CrawlerProcess(settings={
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'CONCURRENT_REQUESTS': 1, # S·ªë request ƒë·ªìng th·ªùi
        'DOWNLOAD_DELAY': 1,      # ƒê·ªô tr·ªÖ gi·ªØa c√°c request
        'ROBOTSTXT_OBEY': True,   # Tu√¢n th·ªß file robots.txt
        'LOG_LEVEL': 'INFO',      # M·ª©c ƒë·ªô log
    })
    
    # Ch·ªçn spider b·∫°n mu·ªën ch·∫°y
    process.crawl(AdvancedQuotesSpider)  # Ho·∫∑c ch·∫°y: QuotesSpiderWithFeeds
    process.start() # Script s·∫Ω d·ª´ng ·ªü ƒë√¢y cho ƒë·∫øn khi crawl xong

    # ƒê·ªçc v√† hi·ªÉn th·ªã k·∫øt qu·∫£ (Ph·∫ßn n√†y ch·ªâ ho·∫°t ƒë·ªông t·ªët v·ªõi AdvancedQuotesSpider)
    try:
        import pandas as pd
        df = pd.read_csv('quotes_data.csv')
        print("\nüìÑ Preview d·ªØ li·ªáu CSV:")
        print(df.head())
    except Exception as e:
        print(f"\n‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng! (Kh√¥ng th·ªÉ preview: {e})")